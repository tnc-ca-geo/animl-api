# Animl architecture

The following documentation describes the data flow and relationships between Animl microservices. Some of the services are managed in separate [repositories](#repositories-and-resources), and this README provides an overview of how they are integrated.

![Animl architecture diagram](/documentation/animl-achitecture-diagram-v1.0.1.png)

## Data flow

Camera trap images can enter Animl in two ways: integrated wireless camera traps can generate new images and push them in real-time into Animl's data processing pipeline ("Data source No. 1", on the left-hand side of the diagram), or users can upload zip files of images directly from their desktops via the frontend UI (Data source No. 2, on the right).

Typically, the latter is used for the processing of images from non-wireless, SD-card based camera traps, from which users will collect large numbers of images–often tens/hundreds of thousands over the course of a deployment-and upload them all at once for inference. Wireless cameras, on the other hand, produce steady streams of small numbers of images as they get triggered in the field, and Animl processes them immediately to support real-time inference and alerts. For more information on the different types of wireless camera traps, how they can be integrated with Animl, and use-cases they are well suited to support, see our [Wireless Camera Trapping Guide](https://guides.animl.camera/).

The entry point for both individual image files and zip files of images uploaded in bulk is the **animl-images-ingestion** S3 Bucket (node No. 6 in the diagram), but the mechanisms for consuming and processing real-time vs bulk-uploaded images differ slightly.

**1. Cellular camera trap data ingestion**

The primary way Animl consumes cellular camera trap data is by intercepting and scraping images from automated email alerts generated by the cameras (this is the case for RidgeTec, Cuddelink, Spartan Cameras, and others). In order to push cellular camera data to Animl from one of these camera trap types, users need to configure them to send email alerts to animl[at]codefornature.org. An SES Receiving Rule intercepts those emails and routes them to the **animl-email-staging** Bucket (2). New objects added to that bucket trigger the **relayImages** lambda function (3), which will scrape the image from the email body or attachments and then drop the image into the **animl-images-ingestion** Bucket (6) to kick off image ingestion and processing.

Animl can also consume cellular camera trap data via a direct API integration in which the camera manufacturers POST new image data to our **animl-ingest-api** Lambda (4), which extracts and then passes the images along to the **animl-images-ingestion** Bucket (6) entrypoint.

**2. Mesh camera trap data ingestion**

Locally-networked, mesh camera traps (like [Buckeye X80s](https://guides.animl.camera/tnc-wireless-camera-trap-documentation/buckeye-x80-networked-wireless-cameras)) can also take photos and transmit them in real-time to an nearby base station (1), which is an internet-connected field computer running the **animl-base** app. **animml-base** watches for new images written to the disk, and when a new image file arrives it uploads it to the **animl-images-ingestion** Bucket (6) entrypoint.

**3. Wireless camera trap data processing**

In all cases above, once images from a wireless camera is added to the ingestion bucket, they follow the same processing pipeline:

1. the **IngestImage** Lambda is triggered (7)
2. their metadata is extracted via the **ExifAPI** (5)
3. the **IngestImage** Lambda resizes the images and moves them to the **animl-images-serving** Bucket for serving
4. that metadata is passed to the **animl-api-graphql** (11), which creates a record of it in the **animl-prod** MongoDB database (12)
5. the **animl-api-graphql** looks up which [Automation Rules](https://docs.animl.camera/fundamentals/automation-rules) to apply to the new image
6. if there's an Automation Rule set to request inference when new images get added to that particular Project, a message requesting inference will be added to the **InferenceQueue** (13). For example, most users will create an Automation Rule to have MegaDetector detect "human", "animal", or "vehicle" detections on all new images added to their Projects.
7. the **animl-api-inference** Lambda handler (14) pulls that message off of the queue requests a prediction from the appropriate Sagemaker Serverless Endpoint (17)
8. if a prediction is returned with a confidence score higher than the user-defined confidence threshold, the **animl-api-inference** Lambda will request a new Label to be created for that Image record from the **animl-api-graphql**. For example, if an "animal" prediction with a high enough confidence was returned from MegaDetector, an "animal" Label would be added to the `Image.labels` array.
9. in addition to each time an Image record is created, the **animl-api-graphql** also checks Automation Rules when new Labels are created. If the user has configured Automation Rules to fire on the creation of that specific Label it will execute them. For example, if the user has an Automation Rule configured to request a second prediction from a species classifier whenever an "animal" Label is created, the **animl-api-graphql** will add another inference request message to the **InferenceQueue** and steps 6 - 9 are repeated.

**3. Bulk uploaded camera trap data ingestion and processing**

When a user uploads a zip file of images to the frontend UI (18), it gets written to the **animl-images-ingestion** Bucket. When the **IngestImage** Lambda (7) detects a zip file has been added to the Bucket, it initiates the **IngestZip** Batch Job, which stands up a temporary Cloudformation stack (SQS queue, Cloudwatch Metric Alarm) for processing that specific bulk upload. The Batch Job validates and extracts the images from the zip file and writes each of them individually to the **images-ingestion** Bucket. When the Cloudwatch Metric Alarm detects that the temporary SQS queue no longer contains messages awaiting processing, the processing is done, and it triggers the **IngestDelete** Lambda which tears down the temporary resources. A more detailed description can be found in the [animl-ingest > IngestZip](#animl-ingest) section of the documentation below.

Like real-time images, we also perform inference on bulk-uploaded images using SageMaker Serverless endpoints. However, we have a separate inference Lambda and a separate set of sagemaker endpoints for processing bulk-uploaded images that have higher concurrency settings than their real-time counterparts to support faster processing of larger workloads. More information can be found in the [animl-api > animl-api-batchinference](#animl-api) section of the documentation below.

## Repositories and resources

### [animl-base](https://github.com/tnc-ca-geo/animl-base)

- Animl Base (1) is a node application deployed on linux-based field computers that ingests new images from a Buckeye wireless camera trap base station and uploads them to S3.

### [animl-email-relay](https://github.com/tnc-ca-geo/animl-email-relay)

- **animl-email-staging -** _S3 bucket_
  - S3 bucket (2) to which SES Receiving rule writes emails sent to animl[at]codefornature.org sent by cellular camera traps
- **relayImages -** _Lambda function_
  - Lambda (3) triggered by new objects added to animl-email-staging bucket. Extracts images in cellular camera trap alert email and adds them to animl-images-ingestion bucket. Additionally may scrape a unique identifier for the camera (e.g. serial number) from the email body or subject line and write it to the images’ exifdata before adding it to the ingestion bucket.

### [animl-ingest-api](https://github.com/tnc-ca-geo/animl-ingest-api)

- **animl-ingest-api -** _Lambda function_
  - Lambda-based API (4) to support consuming cellular camera data via POST requests. Currently, the API only supports image payloads sent by Reconyx's servers.

### [exif-api](https://github.com/tnc-ca-geo/exif-api)

- **exif-api -** _Lambda function_
  - Lambda-based API (5) for exiftool. Extracts EXIF metadata from image files.

### [animl-ingest](https://github.com/tnc-ca-geo/animl-ingest)

- **animl-images-ingestion** - _S3 bucket_
  - primary bucket (6) to which individual images get added for ingestion and processing, as well as zip files of images for bulk processing
- **IngestImage** - _Lambda function_
  - Primary image ingestion handler (7), triggered by new objects added to animl-images-ingestion bucket
  - When new objects are image files:
    - Get metadata from exif-api Lambda
    - Calls `CreateImageRecord` (animl-api-graphql mutation)
    - If successful, resizes the image and moves the copies to animl-images-serving bucket. If not, copies to animl-images-dead-letter bucket.
    - Deletes image from ingestion bucket
  - When new object is a zip file (i.e., when users upload a zip file of images for batch processing):
    - Submits a process-batch Batch Job to the BatchZipIngestQueue, which invokes the BatchComputeEnvironment (Fargate), which loads the ingest-zip docker container image from ECR and runs it
- **IngestZip** - _Batch Job_
  - ingest-zip Batch handler (8) is responsible for creating batch-specific resources, extracting the images from the zip file, moving them to the images-ingestion bucket, and then deleting the batch resources when the batch processing is complete. We create separate, temporary SQS queues for inference for bulk uploads of images so that they don’t block real-time inference requests and so that we can track the processing progress of individual bulk uploads and display progress bars to the user.
  - Specifically, the ingest-zip Batch handler creates a new CloudFormation stack (9) consisting of 2 SQS queues (primary and DLQ) and a Cloudwatch Metric Alarm (stack template is defined in `ingest-zip/lib/stack.js` ). It then downloads the .zip from S3, unzips it, and puts the individual images back in the ingestion bucket for ingestion
    > Note: When copying the image to S3, ingest-zip prepends the extracted images’ keys with 'batch-[batchId]'. This is important because it’s how downstream tasks know whether to process the image as an individual, real-time image or as part of a specific batch. When the ingest-image Lambda parses the Key of newly added images, it checks if the Key matches that template, and if so appends the `batchId` to the images' metadata. The `Image.batchId` is later checked in downstream steps to know what SQS queues to submit inference requests to.
  - The Cloudwatch Metric Alarm gets triggered when there are no more messages left in the batch queue, and it sends a message to the animl-ingest-delete SNS topic
  - Note: there is also a vestigial IngestZip Lambda function created by our Serverless template, but it doesn’t have a trigger and never gets invoked. TBH I can’t remember exactly why that gets created, but I think it had to do with it being the easiest way to build and deploy the IngestZip container image to ECR with Serverless.
- **IngestDelete -** _Lambda function_
  - The ingest-delete Lambda (10) is a subscriber to the animl-ingest-delete SNS topic, so it gets invoked, and it deletes the previously created batch ingestion stack. ingest-delete is also invoked every hour as a cautionary clean-up measure, and it can be invoked via the animl-api when users request stopBatch from the front-end.
- **animl-images-serving** - _S3 bucket_
  - bucket for serving images of three sizes: original, medium, and thumbnails
- **animl-images-dead-letter** - _S3 bucket_
  - bucket for storing images that fail ingestion
- **animl-ingest-delete** - _SNS Topic_

### [animl-api](https://github.com/tnc-ca-geo/animl-api)

- **animl-api-graphql** - _Lambda function_
  - A GraphQL API (11) that serves as the lynchpin for the Animl application and manages the business logic and CRUD operations to a MongoDB Atlas database (12). It has a two paths: an `/external` endpoint that gets called by the animl-frontend UI and is protected by a Cognito authorizer requiring the user's ID token, and an `/internal` endpoint that is called by the animl-api-inference, IngestImage, and other internal Lambda handlers and requires and API key.
    > Note: A full description of the auth strategy can be found here: https://github.com/tnc-ca-geo/animl-api/issues/37
  - Users can configure “Automation Rules” to specify what ML models to request predictions from for their images and under what circumstances - for example, they can create an Automation Rule to request object detections from the MegaDetector model when an image is added to the database, and they could create another to request species-level predictions from a classifier when an “animal” Label is detected and created by MegaDetector.
  - When a new image record is created, the animl-api-graphql hander looks up what Automation Rules are configured for that image’s Project and submits it to the appropriate inference queue (either the inferenceQueue for real-time predictions, or a batch inference queue if the image belongs to a batch/bulk upload)
  - A full list of the supported GraphQL Queries can be found in `src/api/type-defs/root/Query.ts` and a full list of the supported Mutations can be found in `src/api/type-defs/root/Mutation.ts`
- **inferenceQueue** - _SQS queue_
  - Primary SQS queue (13) for real-time inference requests
- **animl-api-inference** - _Lambda function_
  - A Lambda function (14) that pulls real-time inference request messages off of the inferenceQueue SQS queue, requests predictions from the appropriate Sagemaker Serverless endpoint(s), and requests `CreateInternalLabels` from the animl-api-graphql when predictions are returned
- **animl-api-batchinference** - _Lambda function_
  - This Lambda (15) uses the same exact hander code as the real-time inference handler above, the only differences being: it watches for messages in the _batch_ SQS queues, consumes 10 messages at a time instead of 1, and has different reserved concurrency settings so that it can scale out horizontally to make use of 80 concurrent Sagemaker Serveless model endpoints for faster processing of large numbers of images. (The real-time inference Lambda is limited to utilizing 20 concurrent endpoints).
    > Note: A full description of the horizontal scaling approach used to deliver fast, non-blocking processing of batches of images can be found here: https://github.com/tnc-ca-geo/animl-api/issues/101
- **animl-api-task** - _Lambda function_
  - The animl-api-graphql Lambda times out after 30 seconds, so for longer-running tasks that Lambdas are not well suited to support (e.g., situations in which we have to iterate over large numbers of individual records before updating them, or exporting large amounts of data), the animl-api-graphql Lambda will instead send a Task message to the taskQueue, and the actual execution of the task is then handled asynchronously by the animl-api-task Lambda (16), which has a much longer timeout (15 minutes)
    > Note: A full description of the async task handling strategy and examples of long-running tasks can be found here: https://github.com/tnc-ca-geo/animl-api/issues/148
- **taskQueue** - _SQS queue_
  - Queue for long-running task requests
- **animl-exported-data-bucket** - _S3 Bucket_
  - Bucket for writing exported CSVs of image annotation data.

### [animl-ml](https://github.com/tnc-ca-geo/animl-ml)

- A collection of resources to test and deploy computer vision models to SageMaker Serverless Endpoints (17). An example deployment workflow can be found here: https://github.com/tnc-ca-geo/animl-ml/tree/master/api/mirav2

### [animl-frontend](https://github.com/tnc-ca-geo/animl-frontend/)

- **animl-frontend** - _S3 Bucket_
  - A S3 bucket (18) for hosting static React-based frontend app
